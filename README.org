* The Toolkit

The purpose of this toolkit is to help small groups of citizen-hackers join in the [[http://eotarchive.cdlib.org/2016.html][End of Term Web Archive 2016]], a collaborative project hosted by the Internet Archive, California Digital Library, and others.  As concerned citizens, we can use our expertise to fill gaps in the archive and ensure that government data stay archived, open, and usable by the public.  We've written this document to keep track of what we're doing and help other groups with similar aims to ours.

** Why are we doing this?

The End of Term archive webcrawl has run in [[http://eotarchive.cdlib.org/search?f1-administration=2008][previous]] [[http://eotarchive.cdlib.org/search?f1-administration=2012][election years]], and is a non-partisan historical archiving project of great independent value.  However, this year's effort has attracted [[http://www.nytimes.com/2016/12/01/nyregion/harvesting-government-history-one-web-page-at-a-time.html?_r=0][widespread attention]] because of the extreme changes expected with the inauguration of Donald Trump in January.  [[https://technoscienceunit.wordpress.com/2016/12/04/guerrilla-archiving-event-saving-environmental-data-from-trump/][Our perspective]] draws particularly on Canadian experiences under the previous government, which [[http://www.academicmatters.ca/2013/05/harpers-attack-on-science-no-science-no-evidence-no-truth-no-democracy/][greatly restricted access to scientific data]] that conflicted with its policies, and in some cases [[http://www.cbc.ca/news/technology/high-arctic-research-station-forced-to-close-1.1171728][shut down data collection programs]] rather than confront their results. The incoming US administration has already announced its intention to dismantle a number of government programs and agencies.  The [[http://www.nytimes.com/2016/12/07/us/politics/scott-pruitt-epa-trump.html][E.P.A. and climate change research]] are high on the list.

The archiving project thus takes on even greater urgency than it ordinarily would.  We hope that, by identifying and capturing key resources, we can make a real difference in the fight to save science and the environment.

** How it works, and scope of our efforts

The End of Term archiving project is a broad, slow walk through the entire ~.gov~ [[https://en.wikipedia.org/wiki/Top-level_domain][top-level domain]].  It uses an [[https://webarchive.jira.com/wiki/display/Heritrix/Heritrix][open-source webcrawling program]] to visit and archive as many web pages as possible across the whole domain.  However, there are a *lot* of government websites in the US, and many of them are extremely complex.  On its own, the webcrawler will not be able to penetrate deeply into the largest of these sites.

Also, many government resources are not really "web pages"; instead, these documents and data live in complex databases that only serve up the hidden data when someone makes a search query.  In these cases, the webcrawler will *never* capture the data in an archive.  In other cases, static resources are housed in FTP servers. The webcrawler only harvests HTTP pages, so, again, such resources will not be recognized and archived by the webcrawler on its own.  

To capture these resources, we need *manual intervention* in the webcrawler's instructions, and *additional technologies* to get crucial information the webcrawler can't manage.  Our efforts deal mostly with the E.P.A. and other environmental agencies, but we hope that the same techniques can work for NASA, NOAA, NSF, HUD, HHS, and other endangered departments and agencies.  

For now, we see our objectives as the following:
- *identifying* and *flagging* deeply buried sites that link to important troves of HTML documents that the webcrawler will not be likely to find on its own
- *developing recipes* for reverse-engineering URL's of database-stored documents, and potentially *adding metadata* to archived PDF files located in this way
- *locating* non-document-centric datasets (that is, databases collecting large sets of quantitative data instead of texts), *assessing* whether they are in danger of disappearing, and *developing methods* to retrieve and archive the information

So there are three categories of preserveation work here; let's discuss them in turn. 

*** Deep Web Pages: Nominating Seeds

This process is technically straightforward but requires some web research skills. 

*** Text Databases: Reverse-Engineering URL's and Scraping Metadata

We are taking the [[https://nepis.epa.gov/EPA/html/pubs/pubtitle.html][E.P.A. publications list]] as an initial test of this process.  That link goes to a full listing, by title, of all EPA publications currently available on line.  Crawling this space will take a long time but is technically trivial.  However, we lose all metadata by crawling in this way.  The [[https://www.epa.gov/nscep][search page]] here gives one more piece of data -- the year of publication -- which is not much, is actually fairly useful for people who want to make use of them.  What is the best way for us to collect and preserve that metadata? 

Another, slightly less transparent store of documents is the collection of [[https://cdxnodengn.epa.gov/cdx-enepa-public/action/eis/search][Environmental Impact Statements]].  
*** Quantitative Datasets: Finding, filtering, Downloading, and Archiving Them
*** Notes

- step one should be reahing out to the EPA and justasking for the actual data.  That is probably the best thing to do.  

templates for writing to your regional office.

joint force effort between someone knwhoknows something about how the PEA works, and someone who knows how the data is likely to be structured.  






