#+OPTIONS: ':t *:t -:t ::t <:t H:3 \n:nil ^:{} arch:headline author:t
#+OPTIONS: broken-links:nil c:nil creator:nil d:(not "LOGBOOK")
#+OPTIONS: date:t e:t email:nil f:t inline:t num:nil p:nil pri:nil
#+OPTIONS: prop:nil stat:t tags:t tasks:t tex:t timestamp:t title:t
#+OPTIONS: toc:nil todo:t |:t
#+TITLE: Tech Group Goals for Dec. 17, 2016 Guerilla Archiving
#+DATE: <2016-12-16 Fri>
#+AUTHOR: Matt Price
#+EMAIL: matt.price@utoronto.ca
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 26.0.50.1 (Org mode 9.0.1)



In each heading, describe the goal and the desired deliverables. List in priority sequence, if possible.
* Workflow & Scripts for the EPA Publications Collection
It now looks like the IA will harvest all of the EPA *Publications* ([[https://nepis.epa.gov/EPA/html/pubs/pubtitle.html][listed here]])   through the regular webcrawling process.  The webcrawler will *not* retrieve metadata, but that metadata is recorded in extracted textfiles and possibly in PDF's as well.  

Anujan Murugesu has written [[https://github.com/AnujanM/EPADocument-Scraper/blob/master/EPA%20DocID%20Creator.py][a code snippet]] that will do most of the work:
#+BEGIN_SRC python
import re
import requests

docpattern = re.compile("(Dockey=)((\w)*)")
docFile = open("docKey.txt", 'w')
linkFile = open("links.txt", 'w')

dataSite = requests.get("https://nepis.epa.gov/EPA/html/pubs/pubtitle.html").text
matches = docpattern.findall(dataSite)
for match in matches:
    a = match[1]
    fileLink = "https://nepis.epa.gov/Exe/ZyPDF.cgi/" +a + ".PDF?Dockey=" + a + ".pdf"
    docFile.write(a)
    linkFile.write(fileLink)
    linkFile.write("\n")
    docFile.write("\n")
docFile.close()
linkFile.close()
#+END_SRC

** Deliverables

- *Verification:* do the PDF's themselves contain metadata?
- *Code:* write a scraper/crawler that can extract metadata from text & PDF files and save them to a simple DB for the use of future researchers. (mostly done already!)

** Priority
The document collection is very high priority but the IA will almost certainly scrape this by themselves. Flagging this collection as worthwhile and rendering it useful by developing a database are *extrmeely important*, but not *time sensitive*.
* Scraper for the EPA EIS Collection
Environmental Impact Statements or EIS are a crucial tool in environmental regulation.  [[https://cdxnodengn.epa.gov/cdx-enepa-public/action/eis/search][They are accessible here]], and leaving all search criteria blank returns a full listing. Documents are accessible at landing pages two clicks down, which [[https://cdxnodengn.epa.gov/cdx-enepa-II/public/action/eis/details?eisId=223815][look like this]].  These landing pages contain metadata which we should scrape.  

** Deliverables
- *Code:* write a scraper/crawler that can extract metadata form text & PDF files & save them to a simple DB for future researchers.

** Priority
EIS are a high-value document source, but it looks like they will also be retrieved by the official IA crawl. So this is *not* time-sensitive.

* Scraper for Environmental Compliance Documents
[[https://echo.epa.gov/][The Environmental Compliance History database is online here]].These documents are more complex to search, for two reasons:
- not all docs in this db are of the same type
- The query results interface is sort of fnacy and slow, hard to read, and contains lots of metadata.  

** Deliverables
- *Code:* A crawler/scraper that can crawl through the interface and capture as much information as possible, including metadata.  As the IA is unlikely to capture this info, we should ALSO attempt to store all results as WARC files which can be passed on to the IA

** Priority
This is *high priority data* which the IA will *likely not be able to attain*, so this is a *high priority for us.  However, it's harder than the first two.

* FTP Crawling 
Many of the publications and datasets published online by the EPA are served over FTP, [[- FTP crawling: http://cdiac.ornl.gov/data_catalog.html#][e.g., as seen here]]. Can we retrieve these?
** Deliverables
- *Code:* A proof-of-concept FTP crawler for this dataset
- *Text:* Assessment: guide for other collaborators dealing with the same problem
** Priority
Michelle, suggestions?  

* Server Offer Assessment Team
We have received many offers of server space donations, and many suggestions about the appropriate way to store this data (e.g., different protocols, mirroring strategies, etc.). If anyone has subsnatial skills in this area the librarian team could use your help developing a set of options for server strategizing, and criteria for each option.  

** Deliverables
- *Text:* Brief report assessing available options for serving large amounts of data in a sustainable, future-proof, government-proof way.

** Priority
Michelle?

* Protocols for Storage of Quantitative Datasets
The retrieval and storage of quantitative datasets is a complex problem, and currently in flux. The IA is hoping to get direct access to DB dumps or even possibly VM clones for the web interfaces to the currently-available datasets, but *no concrete arrangements have been achieved* and *there is currently no guarantee* that their efforts will be successful.  

We therefore need to answer a number of very urgent questions:
- How do we assess the priority of particular datasets, and target the highest-priority deatasets first?
- What mechanisms are available for dataset capture (e.g., reverse-engineering via web interfaces, partial downloads, and glue code; or, alternatively, walking into an EPA offie and requesting digital copies of the whole thing)?  What are the risks and benefits of each?
- What kind of metadata needs to be associated with each DB we retrieve (e.g., timestamps, provenance, etc.)?

These are complex questions which we will not resolve in our event.  However, we would like to make some progress

** Deliverables
- *Text:* A preliminary rubric outlining answers to these questions, and suggesting some paths towards answers. We'll need help from some library people on this one.  
- *Code:* If possible, we would like to test-run a database retrieval and identify low-hanging fruit as well as potential difficulties.  We would choose a dataset from the [[https://edg.epa.gov/metadata/catalog/search/browse/browse.page][EPA Dataset Gateway]] or a particular dataset identified as high priority.

** Priority
Developmeent of the protocols and stnadards is a *high* priority. Test-driving code is more of a *wishlist*.
